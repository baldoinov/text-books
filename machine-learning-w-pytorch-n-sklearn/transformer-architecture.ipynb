{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Transformer Architecture\n",
    "\n",
    "This notebook was made in the attempt of understanding what is a Transformer (not talking about the movie series here).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. [**_Transformers from Scratch_**](https://e2eml.school/transformers.html)\n",
    "2. [**_The Annotated Transformer._**](http://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "3. [**_Transformer Neural Networks, ChatGPT's foundation, Clearly Explained_**](https://youtu.be/zxQyTK8quyY)\n",
    "4. [Transformer Architecture: The Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)\n",
    "5. [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "6. [Glossary of Deep Learning: Word Embedding](https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca)\n",
    "7. [Fully Connected Layer vs Convolutional Layer: Explained](https://builtin.com/machine-learning/fully-connected-layer)\n",
    "\n",
    "## Extras\n",
    "\n",
    "1. [Attention is all you need; Attentional Neural Network Models | Łukasz Kaiser | Masterclass](https://youtu.be/rBCqOTEfxvg)\n",
    "2. [Attention is all you need (Transformer) - Model explanation (including math), Inference and Training](https://youtu.be/bCz4OMemCcA)\n",
    "3. [Softmax for neural networks](https://e2eml.school/softmax.html)\n",
    "4. [Batch normalization](https://e2eml.school/batch_normalization.html)\n",
    "5. [Intuitive Explanation of Skip Connections in Deep Learning](https://theaisummer.com/skip-connections/)\n",
    "6. [Drawing the Transformer Network from Scratch](https://towardsdatascience.com/drawing-the-transformer-network-from-scratch-part-1-9269ed9a2c5e)\n",
    "7. [Understanding Transformers, the Programming Way](https://towardsdatascience.com/understanding-transformers-the-programming-way-f8ed22d112b2)\n",
    "8. [Implementing the Transformer Encoder from Scratch in TensorFlow and Keras](https://machinelearningmastery.com/implementing-the-transformer-encoder-from-scratch-in-tensorflow-and-keras/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading:\n",
    "\n",
    "Some of those are here because they came before The Tranformer and are useful to understand what kind of problems it solves and some are here to dive deeper in the topic.\n",
    "\n",
    "> Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27.\n",
    "\n",
    "> Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.\n",
    "\n",
    "> Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n",
    "\n",
    "> Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.\n",
    "\n",
    "> Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\n",
    "\n",
    "> Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929–1958, 2014."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "\n",
    "> Word Embedding aims to create a vector representation with a much lower dimensional space. These are called Word Vectors. Word Vectors are used for semantic parsing, to extract meaning from text to enable natural language understanding. For a language model to be able to predict the meaning of text, it needs to be aware of the contextual similarity of words. **_So an answer to “What is word embedding?” is: it’s a means of building a low-dimensional vector representation from corpus of text, which preserves the contextual similarity of words._**\n",
    "\n",
    "> Notice how matrix multiplication acts as a lookup table here. Our A matrix is made up of a stack of one-hot vectors. They have ones in the first column, the fourth column, and the third column, respectively. When we work through the matrix multiplication, this serves to pull out the first row, the fourth row, and the third row of the B matrix, in that order. This trick of using a one-hot vector to pull out a particular row of a matrix is at the core of how transformers work.\n",
    "\n",
    "> This process of selective masking is the attention called out in the title of the original paper on transformers. So far, what we've descibed is a just an approximation of how attention is implemented in the paper. It captures the important concepts, but the details are different. We'll close that gap later.\n",
    "\n",
    "> A good embedding groups words with similar meanings together. A model that works with an embedding learns patterns in the embedded space. That means that whatever it learns to do with one word automatically gets applied to all the words right next to it.\n",
    "\n",
    "> The Mask block enforces the constraint that, at least for this sequence completion task, we can't look into the future. It avoids introducing any weird artifacts from imaginary future words. It is crude and effective - manually set the attention paid to all words past the current position to negative infinity.\n",
    "\n",
    "- Using the attention computation in future words might be the way to induce reasonal thinking?\n",
    "\n",
    "> Another important difference in how attention is implemented is that it makes use of the order in which words are presented to it in the sequence, and represents attention not as a word-to-word relationship, but as a position-to-position relationship.\n",
    "\n",
    "> Skip connections serve two purposes. The first is that they help keep the gradient smooth, which is a big help for backpropagation. The second purpose of skip connections is specific to transformers — preserving the original input sequence.\n",
    "\n",
    "> Having a single attention layer (just one multi-head attention block and one feed forward block) only allows for one path to a good set of transformer parameters. The way transformers sidestep this problem is by having multiple attention layers, each using the output of the previous one as its input.\n",
    "\n",
    "> This is in contrast to the traditional description of many-layered neural networks as \"deep\". Thanks to skip connections, successive layers don't provide increasingly sophisticated abstraction as much as they provide redundancy.\n",
    "\n",
    "> Almost everything we've learned about the decoder applies to the encoder too. The biggest difference is that there's no explicit predictions being made at the end that we can use to judge the rightness or wrongness of its performance. Instead, the end product of an encoder stack is disappointingly abstract—a sequence of vectors in an embedded space. What we know for sure is that it is a useful signal for communicating intent and meaning to the decoder stack.\n",
    "\n",
    "> Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts\n",
    "\n",
    "- What is MLP? How does it work? (Multi-layer Perceptron)\n",
    "\n",
    "**_How fully connected layers work?_**\n",
    "\n",
    "They receive a tensor and perform matrix multiplication to reduce the tensor's dimension. Just remember that the number of units in a tensor can be rearranged so that $W_{\\text{units} \\times N} \\times T_{N \\times M}$\n",
    "\n",
    "**_What are Word Embeddings?_**\n",
    "\n",
    "Word Embeddings are a way to represent the meaning of words in a lower dimension than that of one-hot encoded sentences. After tokenization, a text dataset can be one-hot encoded with a bunch of vectors whose dimension is equal to the size of the vocabulary, that is, `number of dimensions for each vector = number of unique tokens`. The problem is that this representation is not small (see the first and fifth blog post in the listed references to get a comprehensive explanation). A way to overcome this issue is to get a vector representation of the meaning of each word, adding the bonus of having similarly semantic words grouped closer in a lower dimension space. The word embeddings are made through the training of a deep learning model (but we already have good pre-trained models to use).\n",
    "\n",
    "**_How positional encoding works?_**\n",
    "\n",
    "**_Why use skip connections and layer normalization?_**\n",
    "\n",
    "**_What is the Encoder?_**\n",
    "\n",
    "**_What is the Decoder?_**\n",
    "\n",
    "**_What is Attention? What is Self-Attention?_**\n",
    "\n",
    "**_What does the _query, key, and value_ matrix mean? What is the _value_ matrix?_**\n",
    "\n",
    "- What is beam search, and how does it parallelize?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "![](assets/2023-07-26-09-59-20.png)\n",
    "\n",
    "## Multi-Head Attention\n",
    "\n",
    "![](assets/2023-07-26-16-08-56.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Implemetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vitor\\OneDrive\\CODE\\text-books\\machine-learning-w-pytorch-n-sklearn\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import spacy\n",
    "import GPUtil\n",
    "import altair\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torchtext.datasets as datasets\n",
    "\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from os.path import exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecode(nn.Module):\n",
    "    \"\"\"A standard Encoder-Decoder architecture. Base for this and many\n",
    "    other models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        encoded = self.encode(src, src_mask)\n",
    "        return self.decode(encoded, src_mask, tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        embed = self.src_embed(src)\n",
    "        return self.encoder(embed, src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        embed = self.tgt_embed(tgt)\n",
    "        return self.decoder(embed, memory, src_mask, tgt_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        proj = self.proj(x)\n",
    "        return log_softmax(proj, dim=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
