{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Transformer Architecture\n",
    "\n",
    "This notebook was made in the attempt of understanding what is a Transformer (not talking about the movie series here).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Alammar, J. (2018). The Illustrated Transformer. http://jalammar.github.io/illustrated-transformer/\n",
    "- Collis, J. (2017, April 21). Glossary of Deep Learning: Word Embedding. Deeper Learning. https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca\n",
    "- Unzueta, D. (2022, October 18). Fully Connected Layer vs Convolutional Layer: Explained | Built In. Builtin.com. https://builtin.com/machine-learning/fully-connected-layer\n",
    "- (2021). E2eml.school. https://e2eml.school/transformers.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Further reading:\n",
    "\n",
    "Some of those are here because they came before The Tranformer and are useful to understand what kind of problems it solves and some are here to dive deeper in the topic.\n",
    "\n",
    "> Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27.\n",
    "\n",
    "> Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "\n",
    "> Word Embedding aims to create a vector representation with a much lower dimensional space. These are called Word Vectors. Word Vectors are used for semantic parsing, to extract meaning from text to enable natural language understanding. For a language model to be able to predict the meaning of text, it needs to be aware of the contextual similarity of words. **_So an answer to “What is word embedding?” is: it’s a means of building a low-dimensional vector representation from corpus of text, which preserves the contextual similarity of words._**\n",
    "\n",
    "\n",
    "> Notice how matrix multiplication acts as a lookup table here. Our A matrix is made up of a stack of one-hot vectors. They have ones in the first column, the fourth column, and the third column, respectively. When we work through the matrix multiplication, this serves to pull out the first row, the fourth row, and the third row of the B matrix, in that order. This trick of using a one-hot vector to pull out a particular row of a matrix is at the core of how transformers work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts\n",
    "\n",
    "- How fully connected layers work?\n",
    "  - They receive a tensor and perform matrix multiplication to reduce the tensor's dimension. Just remeber that the number of units in a tensor can be rearranged so that $W_{\\text{units} \\times N} \\times T_{N \\times M}$\n",
    "- What is the MLP? How it works? (Multi-layer Perceptron)\n",
    "- What is tokenazation? How it is applied in Trabsformers?\n",
    "- What is word embedding?\n",
    "- What is an Encoder?\n",
    "- What is the Decoder?\n",
    "- What is Self-Attention?\n",
    "- What is beam search and how does it parallelize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart BT\n",
    "    IEB([Input Embedding])\n",
    "    OEB([Output Embedding])\n",
    "    PE1([Positional Encoding +])\n",
    "    PE2([Positional Encoding +])\n",
    "\n",
    "    IEB --> PE1\n",
    "    PE1 --> D1 & C1\n",
    "\n",
    "    OEB --> PE2\n",
    "    PE2 --> E & F\n",
    "\n",
    "    subgraph AttentionHead1[\" \"]\n",
    "        direction BT\n",
    "        A1([Add & Norm])\n",
    "        B1([Feed Forward])\n",
    "        C1([Add & Norm])\n",
    "        D1([Multi-Head Attention])\n",
    "    \n",
    "        D1 --> C1 \n",
    "        C1 --> B1 & A1\n",
    "        B1 --> A1\n",
    "    end\n",
    "\n",
    "    A1 --> D2\n",
    "\n",
    "    subgraph AttentionHead2[\" \"]\n",
    "        direction BT\n",
    "        A2([Add & Norm])\n",
    "        B2([Feed Forward])\n",
    "        C2([Add & Norm])\n",
    "        D2([Multi-Head Attention])\n",
    "        F([Add & Norm])\n",
    "        E([Masked Multi-Head Attention])\n",
    "        \n",
    "        E --> F\n",
    "        F --> D2 & C2\n",
    "        D2 --> C2 \n",
    "        C2 --> B2 & A2\n",
    "        B2 --> A2\n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
