{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Good Training Datasets - Data Preprocessing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts\n",
    "\n",
    "**Mean Imputation**: used for filling missing values in the data, we simply replace missing values with the mean for the entire feature column. Be careful to the presence of outliers, since them skew the mean and don't make it representative of the feature. Other option to interpolate missing values are the **median** and **mode**.\n",
    "\n",
    "**Encoding**:\n",
    "\n",
    "- **One-Hot Encoding**: it is the act of creating dummy features for each unique value in a nominal feature. That way, a column which has three possible values is encoded as, for example, ``[1 0 0]``.\n",
    "  - When using one-hot encoding, we have to keep in mind that this introduces multicollinearity, which can be an issue for certain methods (for instance, those that require matrix inversion) (this is known as the _dummy variable trap_). This is easily solved by dropping one feature.\n",
    "  - It may not be as effective when there are multiple categories in a feature variable or multiple categorical features. In these cases one-hot encoding introduces sparsity in the dataset and don't add much information.\n",
    "\n",
    "- **Binary Encoding**: converts the categories in the feature variable to numerical (using something like the `LabelEncoder`), then the numbers are transformed in their binary representations. After that, the binary value is split in different features. \n",
    "  - This is a specific case of the _Base_ $N$ _Encoding_, with $N = 2$.\n",
    "\n",
    "- **Count or Frequency Encoding**: replaces the label of each category by the number of times or frequency it occurs in the training set.\n",
    "\n",
    "- **Hash Encoding**: represents the categorical data through a hash function. This is useful because hash functions have a fixed output size, so any number of categories can be output to a fixed number of new features. \n",
    "\n",
    "**Feature Scaling**:\n",
    "\n",
    "- **Min-Max Scaling**: refers to scaling the features to a range of ``[0, 1]``, also is called _normalization_. It is accomplished by the following formula: $$x_{norm}^{(i)} = \\frac{x^{(i)} - x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "- **Standardization**: refers to center the feature value around 0 with standard deviation 1, the same parameters as a _standard normal distribution_. It is accomplished by the following formula:  \n",
    "$$\n",
    "x_{std}^{(i)} = \\frac{x^{(i)} - \\mu_x}{\\sigma_x}\n",
    "$$\n",
    "\n",
    "- **Robust Scaler**: refers to scaling each feature column removing the median and scaling the dataset according to the first and third quartiles. This makes more extreme values and outliers less pronounced.\n",
    "  - It is recommended to use it when working with small datasets that contain many outliers. It is also a good choice when the machine learning algorithm applied to the dataset is prone to overfitting.\n",
    "\n",
    "**Regularization**:\n",
    "\n",
    "- Both regularization methods introduce a penalty against model complexity. That is, models with a larger weights are penalized more than those with small weights size. The formulas for each kind of regularization is below.\n",
    "\n",
    "  - **L1 Regularization**: $$||w||_1 = \\sum_{j = 1}^m |w_j|$$ This form of regularization usually yields sparse feature vectors will be zero, which can be usefull for feature selection. \n",
    "  - **L2 Regularization**: $$||w||_2^2 = \\sum_{j = 1}^m w_j^2$$ It is the norm of the weight vector.\n",
    "\n",
    " \n",
    "**Dimensionality Reduction**: it is an alternative way to reduce the complecity of the model and avoid overfitting, which is especially useful for unregularized models.\n",
    "\n",
    "- **Feature Selection**: in this approach, we _select_ a subset of the original features.\n",
    "  - **Sequential Backward Elimination (SBS)**: the SBS algorithm sequentially removes features from the full feature subset until the remaining feature subset has the desired size. To determine which feature is to be removed, a function $J$ is minimized. Usually, this function is the performance difference before and after the removel of a feature.\n",
    "  - **Univariate Statistical Tests**: performs statistical tests in the features to decide which remove.\n",
    "  - **Tree-Based Methods**:\n",
    "\n",
    "- **Feature Extraction**: in this approach, we _derive_ information from the feature set to construct new features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "- `io.StringIO()`\n",
    "- `itertools.combination()`\n",
    "- `pandas.DataFrame().dropna()`\n",
    "- `pandas.DataFrame().fillna()`\n",
    "- `sklearn.impute.SimpleImputer()`\n",
    "- `sklearn.impute.KNNImputer()`\n",
    "- `sklearn.preprocessing.LabelEncoder()`\n",
    "- `sklearn.preprocessing.OneHotEncoder()`\n",
    "- `sklearn.compose.ColumnTransformer()`\n",
    "- `sklearn.model_selection.train_test_split()`\n",
    "- `sklearn.preprocessing.MinMaxScaler()`\n",
    "- `sklearn.preprocessing.StandardScaler()`\n",
    "- `sklearn.preprocessing.RobustScaler()`\n",
    "- `sklearn.feature_selection.SelectFromModel()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- [Model evaluation, model selection, and algorithm selection in machine learning](https://arxiv.org/pdf/1811.12808.pdf)\n",
    "- Section 3.4, The Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, and Jerome Friedman, Springer Science + Business Media, 2009\n",
    "- Comparative Study of Techniques for Large-Scale Feature Selection by F. Ferri, P. Pudil, M. Hatef, and J. Kittler, pages 403-413, 1994"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
