{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "72F5ntFAXoDW"
      },
      "source": [
        "<center>\n",
        "<h1>Deep Learning with Python - Francois Chollet</h1>\n",
        "<h2> Capítulos 01 e 02</h2>\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlvNLuYEXgAo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import datasets\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_25BuTezV7-i",
        "outputId": "b0d05464-b780-4e65-c3a1-a327ad71cc0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 4s 0us/step\n",
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n"
          ]
        }
      ],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n",
        "print(train_images.shape)\n",
        "print(test_images.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "toopSyCdZxMW"
      },
      "source": [
        "***\n",
        "\n",
        "### <center> Trechos do Texto </center>\n",
        "\n",
        "> Learning, in the context of machine learning, describes an automatic search process for data transformations that produce useful representations of some data, guided by some feedback signal.\n",
        "\n",
        "> In machine learning, a category in a classification problem is called a class. Data points are called samples. The class associated with a specific sample is called a label.\n",
        "\n",
        "> The core building block of neural networks is the layer. You can think of a layer as a filter for data: some data goes in, and it comes out in a more useful form\n",
        "\n",
        "> tensors are a generalization of matrices to an arbitrary number of dimensions (note that in the context of tensors, a dimension is often called an axis).\n",
        "\n",
        "> This gap between training accuracy and test accuracy is an example of overfitting: the fact that machine learning models tend to perform worse on new data than on their training data.\n",
        "\n",
        "> *(...) neural networks consist entirely of chains of tensor operations (...) you can interpret a neural network as a very complex geometric transformation in a high-dimensional space, implemented via a series of simple steps.*\n",
        "\n",
        "> ***Backpropagation is simply the application of the chain rule to a computation graph.***\n",
        "\n",
        "> *Learning means finding a set of values for the model's weights that minimizes a loss function.*\n",
        "\n",
        "> What is transformative about deep learning is that it allows a model to learn all layers of representation jointly, at the same time, rather than in succession (greedily, as it’s called) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lZeKY543iGWX"
      },
      "source": [
        "---\n",
        "\n",
        "### <center> Resumo </center>\n",
        "\n",
        "Modelos de ***deep learning*** são construídos com os seguintes items:\n",
        "\n",
        "1. *Layers*\n",
        "  - Os *layers* são uma especíe de filtro para os dados. Eles aplicam transformações que deixam os dados mais úteis para o modelo e os enviam para a próxima camada.\n",
        "2. *Loss function*\n",
        "  - A *função de perda* (*loss function*) é a métrica que buscamos minimizar durante o treinamento do modelo. É o que diz se o modelo está tendo um desempenho bom ou não.\n",
        "3. *Optimizer*\n",
        "  - O *otimizador* é o que diz como o gradiente da função de perda será utilizado para mudar os parâmetros do modelo."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VeR0jfqOh3UT"
      },
      "source": [
        "---\n",
        "\n",
        "#### <center> Tensores </center>\n",
        "\n",
        "Os layers dos modelos são alimentados através de ***tensores***, que são uma generalização de matrizes para um número $n$ de dimensões. Abaixo, podemos ver como imagens coloridas são representadas através de tensores. As primeiras duas dimensões (32 $\\times$ 32) são o tamanho da imagens em pixels, enquanto a última é o e espectro RGB de cada pixel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBqZqRVlWMkJ",
        "outputId": "a2e14afa-ef94-4ff1-c48d-ab5af8daefee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 32, 3)\n",
            "[59 62 63]\t[43 46 45]\t[50 48 43]\t[68 54 42]\t[98 73 52]\t[119  91  63]\t[139 107  75]\t[145 110  80]\t[149 117  89]\t[149 120  93]\t[131 103  77]\t[125  99  76]\t[142 115  91]\t[144 112  86]\t[137 105  79]\t[129  97  71]\t[137 106  79]\t[134 106  76]\t[124  97  64]\t[139 113  78]\t[139 112  75]\t[133 105  69]\t[136 105  74]\t[139 108  77]\t[152 120  89]\t[163 131 100]\t[168 136 108]\t[159 129 102]\t[158 130 104]\t[158 132 108]\t[152 125 102]\t[148 124 103]\n",
            "[16 20 20]\t[0 0 0]\t[18  8  0]\t[51 27  8]\t[88 51 21]\t[120  82  43]\t[128  89  45]\t[127  86  44]\t[126  87  50]\t[116  79  44]\t[106  70  37]\t[101  67  35]\t[105  70  36]\t[113  74  35]\t[109  70  33]\t[112  72  37]\t[119  79  44]\t[109  71  33]\t[105  69  27]\t[125  89  46]\t[127  92  46]\t[122  85  39]\t[131  89  47]\t[124  82  41]\t[121  79  37]\t[131  89  48]\t[132  91  53]\t[133  94  58]\t[133  96  60]\t[123  88  55]\t[119  83  50]\t[122  87  57]\n",
            "[25 24 21]\t[16  7  0]\t[49 27  8]\t[83 50 23]\t[110  72  41]\t[129  92  54]\t[130  93  55]\t[121  82  47]\t[113  77  43]\t[112  78  44]\t[112  79  46]\t[106  75  45]\t[105  73  38]\t[128  92  48]\t[124  87  47]\t[130  92  56]\t[127  89  56]\t[122  85  51]\t[115  79  43]\t[120  85  47]\t[130  95  54]\t[131  96  55]\t[139 102  62]\t[127  90  51]\t[126  89  49]\t[127  89  50]\t[130  92  53]\t[142 105  68]\t[130  94  58]\t[118  84  50]\t[120  84  50]\t[109  73  42]\n",
            "[33 25 17]\t[38 20  4]\t[87 54 25]\t[106  63  28]\t[115  70  33]\t[117  74  35]\t[114  72  37]\t[105  62  33]\t[107  68  33]\t[121  84  45]\t[125  90  53]\t[109  75  40]\t[113  77  38]\t[146 105  58]\t[133  91  47]\t[127  84  45]\t[118  76  40]\t[117  76  41]\t[127  87  52]\t[122  81  43]\t[132  92  51]\t[137  99  58]\t[136  99  57]\t[131  93  52]\t[124  86  44]\t[130  91  50]\t[132  90  49]\t[135  93  51]\t[130  90  50]\t[125  87  50]\t[121  85  48]\t[94 62 35]\n",
            "[50 32 21]\t[59 32 11]\t[102  65  34]\t[127  79  39]\t[124  77  36]\t[121  77  36]\t[120  78  40]\t[114  74  39]\t[107  72  34]\t[125  88  49]\t[129  89  51]\t[106  68  31]\t[108  71  33]\t[124  83  42]\t[121  78  39]\t[108  68  29]\t[98 65 23]\t[110  74  37]\t[117  80  49]\t[120  80  41]\t[134  93  50]\t[140 106  66]\t[131  95  58]\t[141  98  66]\t[135  92  51]\t[127  84  45]\t[121  79  41]\t[119  79  40]\t[103  67  32]\t[87 57 27]\t[75 47 23]\t[67 42 25]\n",
            "[71 48 29]\t[84 53 24]\t[110  73  37]\t[129  82  38]\t[136  88  45]\t[131  84  42]\t[129  84  43]\t[119  77  37]\t[108  70  33]\t[122  82  44]\t[123  81  39]\t[105  65  25]\t[107  72  31]\t[111  77  31]\t[108  74  34]\t[98 65 27]\t[94 62 21]\t[97 63 32]\t[83 56 38]\t[88 58 36]\t[102  68  42]\t[97 69 46]\t[88 54 36]\t[118  74  72]\t[140  96  79]\t[136  97  64]\t[120  80  45]\t[107  68  34]\t[88 54 24]\t[67 39 15]\t[35 10  0]\t[32 13  4]\n",
            "[97 69 40]\t[111  75  36]\t[123  85  43]\t[130  84  38]\t[136  88  44]\t[132  83  40]\t[122  74  30]\t[121  74  31]\t[127  83  46]\t[138  94  54]\t[124  79  34]\t[120  79  39]\t[107  71  34]\t[80 50 14]\t[68 43 17]\t[74 41 17]\t[101  51  21]\t[105  56  23]\t[65 37 16]\t[58 36 19]\t[63 37 18]\t[78 51 31]\t[136  93  83]\t[122  68  80]\t[139  86  79]\t[151 106  69]\t[129  87  49]\t[108  68  36]\t[95 59 29]\t[96 63 37]\t[89 61 38]\t[66 47 30]\n",
            "[115  82  49]\t[119  76  33]\t[130  90  47]\t[140  97  53]\t[133  88  48]\t[127  81  40]\t[138  90  47]\t[137  89  46]\t[131  86  48]\t[133  89  46]\t[134  91  46]\t[108  70  39]\t[72 39 19]\t[51 26 10]\t[41 22 14]\t[72 31 17]\t[181 102  69]\t[209 127  81]\t[125  76  47]\t[68 40 23]\t[64 38 17]\t[82 53 30]\t[123  77  62]\t[112  56  55]\t[135  81  60]\t[151 103  61]\t[137  95  54]\t[114  76  39]\t[105  69  34]\t[101  66  33]\t[126  92  59]\t[102  74  46]\n",
            "[137 100  68]\t[128  82  41]\t[132  91  51]\t[128  87  48]\t[119  81  44]\t[123  82  43]\t[128  85  44]\t[130  85  44]\t[121  80  40]\t[137  97  54]\t[131  94  53]\t[74 42 20]\t[54 25 16]\t[50 29 16]\t[44 29 18]\t[86 39 15]\t[203 106  56]\t[217 109  62]\t[162  90  71]\t[100  58  49]\t[77 42 27]\t[75 43 24]\t[74 39 24]\t[76 35 22]\t[107  67  36]\t[135  96  59]\t[135  97  58]\t[129  91  49]\t[127  89  48]\t[119  83  43]\t[125  86  45]\t[134  95  56]\n",
            "[154 120  89]\t[154 112  77]\t[156 114  82]\t[140 100  65]\t[123  89  53]\t[125  86  50]\t[126  86  48]\t[127  91  52]\t[133  97  60]\t[132  97  68]\t[90 60 30]\t[63 35  9]\t[62 33 16]\t[70 39 20]\t[79 50 30]\t[103  53  26]\t[152  70  33]\t[148  64  37]\t[141  79  61]\t[121  75  57]\t[101  58  41]\t[96 54 33]\t[86 48 24]\t[75 38 21]\t[101  63  32]\t[136  91  53]\t[136  92  53]\t[134  93  50]\t[133  93  52]\t[132  93  52]\t[128  86  45]\t[133  92  55]\n",
            "[154 122  94]\t[155 117  82]\t[156 117  82]\t[147 108  70]\t[133 100  64]\t[137 100  66]\t[139 102  68]\t[134 102  66]\t[141 111  81]\t[121  87  68]\t[80 40 13]\t[97 53 17]\t[90 45 17]\t[98 56 30]\t[137  91  57]\t[139  84  49]\t[148  87  54]\t[134  73  37]\t[138  82  46]\t[134  85  57]\t[140  92  76]\t[175 129 106]\t[142  99  53]\t[102  61  26]\t[108  67  25]\t[135  90  41]\t[131  87  45]\t[133  91  51]\t[138  97  57]\t[136  95  55]\t[130  86  46]\t[134  93  57]\n",
            "[145 114  89]\t[146 109  73]\t[146 109  69]\t[135  97  55]\t[127  92  57]\t[129  94  65]\t[117  84  55]\t[103  74  42]\t[130 103  70]\t[120  83  55]\t[111  60  14]\t[146  86  22]\t[136  78  23]\t[163 116  77]\t[169 115  69]\t[152 100  52]\t[161 116  73]\t[148  97  57]\t[177 121  82]\t[161 110  71]\t[195 150 113]\t[209 167 123]\t[189 146  94]\t[125  78  40]\t[108  63  25]\t[140  96  52]\t[137  95  59]\t[132  93  56]\t[136  95  57]\t[133  90  51]\t[132  87  46]\t[133  92  56]\n",
            "[142 115  86]\t[141 106  69]\t[140 105  68]\t[144 105  64]\t[147 110  74]\t[121  89  65]\t[84 56 34]\t[88 61 33]\t[109  80  44]\t[101  57  23]\t[138  79  19]\t[213 150  59]\t[178 123  41]\t[191 150  98]\t[211 169 122]\t[189 148  99]\t[205 164 110]\t[207 162 115]\t[213 164 118]\t[191 143  91]\t[199 158  97]\t[188 151  88]\t[161 121  76]\t[130  83  50]\t[124  77  38]\t[131  87  51]\t[130  91  61]\t[131  93  60]\t[134  93  57]\t[135  91  52]\t[136  89  48]\t[133  91  56]\n",
            "[158 131  98]\t[154 119  82]\t[142 107  74]\t[143 102  65]\t[132  92  59]\t[90 59 36]\t[72 44 22]\t[81 52 24]\t[84 47 19]\t[107  55  25]\t[165 106  50]\t[229 176  92]\t[183 137  57]\t[191 158 103]\t[239 216 176]\t[219 192 149]\t[228 188 128]\t[225 188 120]\t[214 177 112]\t[216 174 112]\t[210 171 110]\t[200 169 109]\t[189 162 114]\t[174 137 100]\t[161 118  76]\t[139  95  57]\t[134  96  66]\t[126  90  59]\t[131  92  56]\t[142  98  60]\t[136  89  48]\t[138  97  61]\n",
            "[145 115  79]\t[149 109  66]\t[147 108  68]\t[147 105  65]\t[136  95  62]\t[80 47 21]\t[89 57 32]\t[105  68  40]\t[96 51 26]\t[129  81  45]\t[192 152 113]\t[185 148 107]\t[145 101  51]\t[203 162 121]\t[223 200 170]\t[242 227 196]\t[244 227 186]\t[238 220 165]\t[241 219 163]\t[227 197 144]\t[225 191 139]\t[235 209 157]\t[219 206 164]\t[224 208 181]\t[215 192 156]\t[156 118  78]\t[128  89  57]\t[129  95  62]\t[131  95  60]\t[133  97  60]\t[128  89  50]\t[130  92  56]\n",
            "[148 116  79]\t[146 100  54]\t[145 100  55]\t[147 100  51]\t[133  96  54]\t[63 42 21]\t[66 43 31]\t[88 50 34]\t[113  65  37]\t[182 146 110]\t[220 191 169]\t[138  94  71]\t[162 105  63]\t[206 156 112]\t[196 166 135]\t[247 234 212]\t[255 253 232]\t[255 252 219]\t[245 234 197]\t[236 217 180]\t[230 208 170]\t[215 196 160]\t[231 217 197]\t[250 241 229]\t[241 229 195]\t[158 132  78]\t[125  95  49]\t[126  97  58]\t[124  92  52]\t[125  91  49]\t[126  88  46]\t[124  88  54]\n",
            "[149 115  79]\t[143  95  49]\t[144  97  51]\t[151  99  51]\t[132  87  49]\t[64 40 21]\t[84 59 41]\t[112  69  37]\t[163 121  75]\t[223 204 166]\t[206 182 157]\t[145  90  56]\t[196 133  84]\t[204 157 110]\t[220 188 156]\t[243 226 208]\t[245 237 226]\t[239 233 215]\t[234 224 201]\t[231 217 192]\t[195 181 152]\t[150 137 100]\t[208 193 154]\t[250 241 216]\t[227 216 173]\t[163 142  78]\t[145 127  60]\t[143 129  62]\t[140 123  55]\t[136 116  46]\t[121  95  30]\t[114  82  40]\n",
            "[147 111  76]\t[134  88  47]\t[140  99  61]\t[148 103  66]\t[135  89  60]\t[100  64  38]\t[108  73  43]\t[144 104  66]\t[210 181 140]\t[248 243 212]\t[175 147 115]\t[175 119  73]\t[220 176 129]\t[226 197 164]\t[230 207 179]\t[233 218 196]\t[224 212 195]\t[201 186 166]\t[184 163 138]\t[181 158 128]\t[190 171 136]\t[170 157 105]\t[179 167 105]\t[231 218 181]\t[223 206 161]\t[162 133  71]\t[146 116  43]\t[140 115  34]\t[139 116  33]\t[145 123  38]\t[142 119  35]\t[128 102  41]\n",
            "[152 114  80]\t[117  75  37]\t[114  80  48]\t[123  90  57]\t[126  91  56]\t[122  83  48]\t[93 58 32]\t[179 154 138]\t[238 226 212]\t[248 243 229]\t[170 134 104]\t[185 132  88]\t[241 214 177]\t[230 218 195]\t[187 169 142]\t[180 160 131]\t[166 146 115]\t[146 119  85]\t[149 116  79]\t[157 124  83]\t[184 157 110]\t[216 195 141]\t[212 198 152]\t[236 221 197]\t[236 212 176]\t[166 125  63]\t[136  85  16]\t[134  81  13]\t[130  83  13]\t[127  86  16]\t[137 105  27]\t[151 128  54]\n",
            "[145 105  72]\t[127  82  41]\t[128  90  51]\t[133  92  53]\t[132  89  49]\t[135  95  51]\t[171 145 110]\t[237 227 205]\t[252 247 235]\t[229 213 194]\t[173 136 100]\t[169 121  73]\t[220 182 138]\t[194 169 135]\t[123  89  55]\t[135  98  60]\t[127  91  48]\t[151 114  63]\t[165 127  74]\t[132  99  50]\t[151 126  79]\t[202 183 142]\t[240 228 203]\t[240 225 210]\t[222 196 169]\t[156 117  64]\t[119  76  12]\t[120  75  16]\t[112  66  14]\t[100  65  15]\t[99 74 19]\t[140 121  54]\n",
            "[143 104  66]\t[127  80  38]\t[129  86  49]\t[129  85  46]\t[130  86  45]\t[140 102  59]\t[219 196 161]\t[244 232 210]\t[210 199 186]\t[193 173 151]\t[166 129  92]\t[153 104  55]\t[191 146  96]\t[179 145 105]\t[128  86  47]\t[147 102  58]\t[149 106  59]\t[172 131  78]\t[147 108  54]\t[128  94  45]\t[141 113  67]\t[173 150 112]\t[202 183 160]\t[190 171 147]\t[198 175 146]\t[152 124  86]\t[100  72  26]\t[109  81  34]\t[119  88  43]\t[121  92  50]\t[108  82  36]\t[136 119  50]\n",
            "[143 104  64]\t[125  76  32]\t[131  85  48]\t[128  81  43]\t[123  81  39]\t[153 117  76]\t[148 118  85]\t[166 141 118]\t[188 166 147]\t[182 156 132]\t[171 134  99]\t[165 115  69]\t[195 148  99]\t[190 153 110]\t[152 108  66]\t[143  95  49]\t[152 105  56]\t[153 110  58]\t[142 102  51]\t[141 102  54]\t[135 101  55]\t[136 101  60]\t[148 110  74]\t[141 106  65]\t[141 111  68]\t[138 113  71]\t[111 100  37]\t[111 111  31]\t[121 118  35]\t[129 116  39]\t[138 116  45]\t[179 162  83]\n",
            "[141 102  65]\t[131  80  35]\t[139  89  46]\t[139  87  44]\t[138  90  50]\t[151 111  71]\t[128  91  52]\t[136  97  61]\t[175 136 104]\t[173 136 107]\t[189 151 118]\t[205 160 120]\t[201 157 113]\t[168 131  89]\t[151 108  65]\t[145  97  53]\t[146 101  52]\t[149 106  57]\t[153 110  61]\t[149 108  61]\t[144 104  59]\t[144 105  59]\t[145 104  59]\t[143 102  60]\t[129  96  48]\t[123 103  39]\t[124 126  30]\t[113 135  14]\t[108 133   8]\t[113 122  10]\t[148 136  44]\t[199 184 102]\n",
            "[143 103  72]\t[139  87  44]\t[138  89  42]\t[149  96  52]\t[160 109  72]\t[150 106  64]\t[147 104  58]\t[151 104  57]\t[169 121  81]\t[167 123  87]\t[179 141 105]\t[212 174 138]\t[203 168 132]\t[207 177 141]\t[149 112  74]\t[139  96  55]\t[144 102  56]\t[137  94  47]\t[151 107  61]\t[155 111  65]\t[152 109  63]\t[140 101  55]\t[107  76  38]\t[91 60 34]\t[84 61 23]\t[105  99  25]\t[132 142  34]\t[118 141  20]\t[ 96 121   4]\t[102 113   9]\t[159 149  63]\t[190 174  99]\n",
            "[149 107  74]\t[133  80  37]\t[136  88  48]\t[147  99  59]\t[150 104  63]\t[153 109  62]\t[157 112  67]\t[162 117  78]\t[175 131  96]\t[190 145 107]\t[166 124  84]\t[202 168 133]\t[224 197 168]\t[197 175 148]\t[192 165 133]\t[180 144 107]\t[146 106  64]\t[126  82  40]\t[141  97  52]\t[156 112  66]\t[153 109  61]\t[115  74  29]\t[77 43 14]\t[79 50 25]\t[93 73 33]\t[126 117  47]\t[133 134  39]\t[119 116  31]\t[113  99  24]\t[140 121  42]\t[187 165  91]\t[154 132  75]\n",
            "[172 128  76]\t[144  88  18]\t[135  85  35]\t[136  88  48]\t[135  90  45]\t[139  94  49]\t[153 108  68]\t[163 117  83]\t[166 120  82]\t[184 136 100]\t[166 118  88]\t[150 110  75]\t[184 149 110]\t[156 121  86]\t[158 123  87]\t[168 130  92]\t[149 109  71]\t[135  91  51]\t[130  85  43]\t[132  88  43]\t[128  84  37]\t[127  83  36]\t[135  92  52]\t[143 105  66]\t[139 108  62]\t[136 112  52]\t[127 105  39]\t[121  92  39]\t[135 102  44]\t[189 159  87]\t[211 181 114]\t[136 107  58]\n",
            "[202 157  82]\t[187 129  26]\t[151 100  25]\t[128  79  34]\t[122  76  41]\t[134  88  49]\t[142  98  53]\t[150 106  56]\t[153 106  58]\t[148  99  63]\t[135  87  59]\t[127  82  44]\t[153 109  60]\t[166 121  77]\t[143  99  59]\t[130  88  51]\t[128  87  52]\t[151 108  70]\t[152 106  65]\t[135  90  48]\t[139  95  50]\t[155 110  63]\t[161 113  65]\t[154 107  63]\t[154 112  67]\t[143 105  54]\t[130  93  44]\t[132  90  46]\t[171 131  70]\t[215 183 106]\t[186 155  91]\t[117  86  48]\n",
            "[216 174  87]\t[193 136  16]\t[168 122  19]\t[151 111  35]\t[131  88  34]\t[126  82  35]\t[138  94  49]\t[144 100  53]\t[142  95  53]\t[137  92  51]\t[120  78  34]\t[131  87  41]\t[145  99  52]\t[144 101  57]\t[137  94  54]\t[127  83  48]\t[126  82  51]\t[139  94  60]\t[153 108  69]\t[149 104  63]\t[140  95  53]\t[135  91  46]\t[147 103  57]\t[148 105  59]\t[149 108  62]\t[149 109  63]\t[137 101  54]\t[143 107  57]\t[203 167 102]\t[206 173 105]\t[124  93  49]\t[71 48 26]\n",
            "[220 182  91]\t[201 150  22]\t[186 148  24]\t[172 139  28]\t[156 120  26]\t[142 103  30]\t[142 100  51]\t[153 108  75]\t[150 105  73]\t[139  98  57]\t[126  88  38]\t[136  92  47]\t[148 102  62]\t[141 101  60]\t[131  89  51]\t[126  82  49]\t[127  81  51]\t[138  88  52]\t[150 100  60]\t[154 104  65]\t[149 101  60]\t[124  78  36]\t[126  85  41]\t[141 101  55]\t[145 107  61]\t[147 112  68]\t[127 101  59]\t[114  87  46]\t[186 155  98]\t[173 144  87]\t[56 29  9]\t[33 19  9]\n",
            "[208 170  96]\t[201 153  34]\t[198 161  26]\t[191 157  27]\t[183 146  34]\t[171 135  32]\t[159 121  42]\t[147 107  52]\t[135  95  49]\t[130  87  46]\t[139  93  57]\t[147  98  62]\t[144  95  55]\t[145  99  57]\t[137  91  51]\t[136  89  52]\t[137  90  54]\t[148 102  58]\t[152 106  60]\t[150 103  61]\t[155 110  64]\t[138  94  46]\t[120  76  33]\t[128  84  39]\t[142 102  58]\t[135 103  62]\t[90 69 40]\t[50 24 11]\t[137 105  60]\t[160 133  70]\t[56 31  7]\t[53 34 20]\n",
            "[180 139  96]\t[173 123  42]\t[186 144  30]\t[194 153  25]\t[198 158  34]\t[201 164  36]\t[189 153  32]\t[173 137  32]\t[156 118  38]\t[139  99  38]\t[142  97  49]\t[145  97  56]\t[141  92  52]\t[141  93  52]\t[139  91  51]\t[140  91  53]\t[143  95  58]\t[139  99  60]\t[138  98  60]\t[143  96  56]\t[146  93  43]\t[135  84  33]\t[117  80  38]\t[112  72  29]\t[122  81  39]\t[104  67  30]\t[58 31 11]\t[34  5  0]\t[131  94  57]\t[184 148  94]\t[97 62 34]\t[83 53 34]\n",
            "[177 144 116]\t[168 129  94]\t[179 142  87]\t[188 149  67]\t[202 168  68]\t[218 189  76]\t[218 191  72]\t[207 181  70]\t[191 163  79]\t[175 143  82]\t[166 132  86]\t[163 128  92]\t[163 127  94]\t[161 123  92]\t[153 114  84]\t[159 120  90]\t[162 124  93]\t[149 116  91]\t[140 104  83]\t[148 103  77]\t[161 105  69]\t[144  95  55]\t[112  90  59]\t[119  91  58]\t[130  96  65]\t[120  87  59]\t[92 67 46]\t[103  78  57]\t[170 140 104]\t[216 184 140]\t[151 118  84]\t[123  92  72]\n"
          ]
        }
      ],
      "source": [
        "print(train_images[0].shape)\n",
        "print('\\n'.join(['\\t'.join([str(cell) for cell in row]) for row in train_images[0]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxdmpWp7WXC6",
        "outputId": "ce380961-bb7b-499a-d89b-8b4d362ca870"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 3)\n",
            "25\t24\t21\n",
            "16\t7\t0\n",
            "49\t27\t8\n",
            "83\t50\t23\n",
            "110\t72\t41\n",
            "129\t92\t54\n",
            "130\t93\t55\n",
            "121\t82\t47\n",
            "113\t77\t43\n",
            "112\t78\t44\n",
            "112\t79\t46\n",
            "106\t75\t45\n",
            "105\t73\t38\n",
            "128\t92\t48\n",
            "124\t87\t47\n",
            "130\t92\t56\n",
            "127\t89\t56\n",
            "122\t85\t51\n",
            "115\t79\t43\n",
            "120\t85\t47\n",
            "130\t95\t54\n",
            "131\t96\t55\n",
            "139\t102\t62\n",
            "127\t90\t51\n",
            "126\t89\t49\n",
            "127\t89\t50\n",
            "130\t92\t53\n",
            "142\t105\t68\n",
            "130\t94\t58\n",
            "118\t84\t50\n",
            "120\t84\t50\n",
            "109\t73\t42\n"
          ]
        }
      ],
      "source": [
        "print(train_images[0][2].shape)\n",
        "print('\\n'.join(['\\t'.join([str(cell) for cell in row]) for row in train_images[0][2]]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KMDcJzkvjnqo"
      },
      "source": [
        "Os tensores performam as mesmas operações que as matrizes, mas algumas coisas que chamam atenção, são:\n",
        "\n",
        "- *Broadcasting*: é completar uma matriz para que uma operação com outra matriz seja viável. No exemplos abaixo, vemos a adição de duas matrizes tal que: $$X_{3 \\times 3} + y_{1 \\times 3} \\xrightarrow{Broadcasting} X_{3 \\times 3} + Y_{{3 \\times 3}}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFsMFzw_Zy05",
        "outputId": "d40e77c5-5bab-4fa7-fab0-0a5e666583f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matrizes X e y originais:\n",
            "\n",
            "\n",
            "0.017404007623929685\t0.757174364606277\t0.4590071296361412\n",
            "0.4692090721022908\t0.1664666125848685\t0.27262222603439035\n",
            "0.1926766946943067\t0.9821710966619618\t0.9770286625319604\n",
            "\n",
            "\n",
            "[0.97246745 0.54651047 0.31648371]\n",
            "\n",
            "\n",
            "Matrizes X e Y após a operação:\n",
            "\n",
            "\n",
            "0.017404007623929685\t0.757174364606277\t0.4590071296361412\n",
            "0.4692090721022908\t0.1664666125848685\t0.27262222603439035\n",
            "0.1926766946943067\t0.9821710966619618\t0.9770286625319604\n",
            "\n",
            "\n",
            "0.9724674519508465\t0.5465104707225186\t0.316483714154691\n",
            "0.9724674519508465\t0.5465104707225186\t0.316483714154691\n",
            "0.9724674519508465\t0.5465104707225186\t0.316483714154691\n"
          ]
        }
      ],
      "source": [
        "X = np.random.random((3, 3))\n",
        "y = np.random.random((3,))\n",
        "\n",
        "print(\"Matrizes X e y originais:\\n\\n\")\n",
        "print('\\n'.join(['\\t'.join([str(cell) for cell in row]) for row in X]))\n",
        "print(\"\\n\")\n",
        "print(y)\n",
        "\n",
        "y = np.expand_dims(y, axis=0)\n",
        "Y = np.concatenate([y] * 3, axis=0)\n",
        "\n",
        "print(\"\\n\\nMatrizes X e Y após a operação:\\n\\n\")\n",
        "print('\\n'.join(['\\t'.join([str(cell) for cell in row]) for row in X]))\n",
        "print(\"\\n\")\n",
        "print('\\n'.join(['\\t'.join([str(cell) for cell in row]) for row in Y]))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ostSiQ71mJab"
      },
      "source": [
        "- Multiplicação de tensores: segundo [essa resposta do *math exchange*](https://math.stackexchange.com/questions/63074/is-there-a-3-dimensional-matrix-by-matrix-product), a multiplicação de tensores em dimensões maiores do que 2 funcionaria da seguinte forma: $$c_{il} = \\sum_{j,k} a_{ijk} \\cdot b_{kjl} $$\n",
        "  - Na wikipedia essa operação está descrita como [*tensor contraction*](https://en.wikipedia.org/wiki/Tensor_contraction)\n",
        "\n",
        "- Operações com tensores: algumas operações com tensores possuem uma representaa mais específica. Algumas delas são (ver características no livro):\n",
        "  - Translação\n",
        "  - Rotação\n",
        "  - Escalonamento"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "X764387ewDi5"
      },
      "source": [
        "---\n",
        "\n",
        "#### <center>Gradiente</center>\n",
        "\n",
        "Minimizar o erro de uma rede neural é minimizar uma função de custo $f: \\mathbb{R^n} \\rightarrow \\mathbb{R}$. Por se tratar de uma minimização, utilizamos o ***gradiente*** para navegar a superfície de erro até o ponto mínimo. O gradiente de uma função é dado por: $$\\nabla f(x_1, \\dots, x_n) = \\left (\\frac{\\partial f }{\\partial x_1}, \\dots, \\frac{\\partial f}{\\partial x_n} \\right )$$\n",
        "\n",
        "Quando rodamos o modelo a primeira vez (lembrando que os pesos são iniciados de forma aletória) e conseguimos o primeiro conjunto de respostas ($\\hat{y}$), temos o ***forward pass***. Após calcularmos o erro do modelo (o quão longe $\\hat{y}$ ficou de $y$), fazemos o ***backward pass***, que é utilizar o gradiente do função de erro (como os pesos da rede alteram o erro) para descobrir como alterar os pesos e diminuir o erro. A medida de alteração nos pesos é a ***learning rate***. O ato de usar o erro para atualizar todos os pesos da rede é o algoritmo de ***backpropagation***, e consiste em utilizar a *regra da cadeia* para saber como as camadas escondidas iniciais (e todas as outras) da rede impactam no erro."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hUr7EfaI0dX1"
      },
      "source": [
        "#### <center>Modelo Implementado com Tensorflow</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGvQBvParGJy"
      },
      "outputs": [],
      "source": [
        "# Exemplo de modelo implementado à \"mão\" com TensorFlow.\n",
        "\n",
        "from math import ceil\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class NaiveDense:\n",
        "  \"\"\"Dense layer\"\"\"\n",
        "\n",
        "  def __init__(self, input_size, output_size, activation):\n",
        "    self.activation = activation\n",
        "  \n",
        "    w_shape = (input_size, output_size)\n",
        "    w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n",
        "    self.W = tf.Variable(w_initial_value) # Weights\n",
        "\n",
        "    b_shape = (output_size,)\n",
        "    b_initial_value = tf.zeros(b_shape)\n",
        "    self.b = tf.Variable(b_initial_value) # Bias\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "    return self.activation(tf.matmul(inputs, self.W) + self.b)\n",
        "  \n",
        "  @property # Usado para transformar `weights` em uma propriedade da classe\n",
        "  def weights(self):\n",
        "    return [self.W, self.b]\n",
        "\n",
        "class NaiveSequential:\n",
        "  \"\"\"Empilhando layers\"\"\"\n",
        "\n",
        "  def __init__(self, layers):\n",
        "    self.layers = layers\n",
        "  \n",
        "  def __call__(self, inputs):\n",
        "    x = inputs\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x\n",
        "  \n",
        "  @property\n",
        "  def weights(self):\n",
        "    weights = []\n",
        "    for layer in self.layers:\n",
        "      weights = weights + layer.Weights\n",
        "    return weights\n",
        "\n",
        "class BatchGenerator:\n",
        "  \n",
        "  def __init__(self, images, labels, batch_size=120):\n",
        "    assert len(images) == len(labels)\n",
        "    self.index = 0\n",
        "    self.images = images\n",
        "    self.labels = labels\n",
        "    self.batch_size = batch_size/\n",
        "    self.num_batches = ceil(len(images) / batch_size)\n",
        "  \n",
        "  def next(self):\n",
        "    images = self.images[self.index: self.index + self.batch_size]\n",
        "    labels = self.labels[self.index: self.index + self.batch_size]\n",
        "    \n",
        "    self.index = self.index + self.batch_size\n",
        "    return images, labels"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dy_s7Ew4r4nK"
      },
      "source": [
        "---\n",
        "\n",
        "### <center> Dúvidas </center>\n",
        "\n",
        "- Não entendi muito bem o método de otimização usando *momentum*. O senhor pode passar a intuição dele?\n",
        "- A intuição por trás do algoritmo de *backpropagation* ficou meio confusa para mim na parte de inverter o grafo computacional para achar como os pesos contribuem para a variação na *loss function*. Pode explicar de novo?\n",
        "- Os dados são distribuído em batchs para cada nó do modelo, certo? A distribuição inicial pode alterar o resultado final do modelo?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoR2nrU7vRgn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
